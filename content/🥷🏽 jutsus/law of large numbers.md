---
date: 2024-11-24 19:53
sources: "[[probability]]"
tags:
  - zettel
  - data-science
status: literature
publish: true
---
# Law of Large Numbers

It is a phenomenon in [[probability]] where more the number of same experiments that are conducted more closer the [[probability]] gets to the expected [[probability]]. 

Let's take an example of `tossing a coin`. When we toss a coin we know the expected [[probability]] of getting `heads` is 50%. But let's say we toss a coin twice, both the times we get `tails`(this indicates the expected [[probability]] is 100% tails and not 50% heads). But when we run these experiments for a large number of times then it would get closer to the expected [[probability]]. 

---
## Related Notes
[[probability]]

## References(links)
[[Core ML and MLOPs]]